## 此為專題之內容簡介
介紹影片(以Yolov8x作介紹)： https://www.youtube.com/watch?v=Ba3QV_6B4h8

題目主旨：台灣的交通問題日益嚴重，其中一個不容忽視的問題是駕駛人亂丟垃圾的行為。這種亂象將增加城市街道和公共場所的垃圾堆積量，不僅破壞的城市景觀，還可能導致騎乘人員的安全隱患，引發交通事故。本計畫旨在解決台灣行車時亂丟垃圾之行為，設計與實作一套『垃圾監測違規辨識系統』，該系統將透過物件辨識以及深度學習技術，準確辨識駕駛的違規亂丟垃圾行為，並記錄該車輛的車牌，以此警示民眾不要因貪求方便而做出違規丟棄垃圾之行為。
---
運用技術：使用DeepSort將不同幀之間的相同物件視為同一物件，並給予編號，透過我們團隊設定的事件處理，當畫面中的物件在短時間內Y軸距離移動超過我們所設定的動態閥值，將會去回朔該物件在每一幀中的位置，並且根據移動的X值速度和方向，回推出是哪位叫使人所丟棄之垃圾，最終呼叫車牌辨識模型，並回傳給使用者。動態閥值的設定是根據垃圾物件出現時，其最近的交通工具方框的大小所決定。若越小，則閥值越低。這是因為我們發現，當畫面距離越遠時，垃圾掉落相對於畫面的Y值幅度會較小。然而近距離時，垃圾相對於畫面的Y值幅度會較大。若使用相同的標準將易錯判垃圾為掉落狀態與否，因此我們根據其最近的交通工具在畫面中的大小去設定閥值以達到更好的效果。
---
使用模型：共分為Yolov8m、yolov8x、yolov9-C、yolov9-E、PRB-FPN 此五種模型針對我們團隊所自行label的dataset進行訓練，得出來的mean Average Precision(mAP)各為下圖，可以發現yolov9-E的mAP相較於yolov8x和PRB-FPN來說還要低，然而，再我們實際測量時，卻發現yolov9-E反而具備優秀的物件追蹤能力及穩定性，因此最後本系統採用Yolov9-E作為核心模型。
Yolov8m
![1](https://github.com/user-attachments/assets/e2c9d2b3-7073-4e2f-a280-c799be4e86ac)
Yolov8x
![2png](https://github.com/user-attachments/assets/19ade3bc-41d7-405f-955a-8ae5fa533aa2)
Yolov9-c
![3](https://github.com/user-attachments/assets/7d05549a-7a12-427c-a10b-607c95396af2)
Yolov9-E
![4](https://github.com/user-attachments/assets/889e3320-7bb6-4143-9938-4818ab800947)
PRB-FPN
![5](https://github.com/user-attachments/assets/fa943149-cd99-4d89-9b4f-fa1042b8841c)
---
下面為專題實作方式
![6](https://github.com/user-attachments/assets/3e9bf6d6-52f6-459e-bf5e-8e92ad7e7e32)
專題成果流程圖
![7](https://github.com/user-attachments/assets/1a8566a9-9a7b-456d-a188-78d0a54a5a5e)
---

以下為專題之較詳細內容：
專題簡介	    
台灣的交通問題日益嚴重，其中一個不容忽視的問題是駕駛人亂丟垃圾的行為。這種亂象將增加城市街道和公共場所的垃圾堆積量，不僅破壞城市景觀，還可能導致騎乘人員的安全隱患，甚至引發交通事故。
本專題旨在解決台灣行車時亂丟垃圾之行為，設計與實作一套『垃圾監測違規辨識系統』。系統以影片作為輸入，透過物件辨識以及深度學習技術，精確辨識駕駛人亂丟垃圾的違規行為，並記錄該車輛的車牌，以此警示民眾不要因貪圖方便而做出違規丟棄垃圾之行為。
這個專題使用團隊標註的2600多張垃圾照片，對YOLOv8、YOLOv9和PRB-FPN等模型進行訓練，以比較它們在識別標註照片中的不同垃圾種類(包括塑膠瓶、鋁罐、塑膠袋和紙杯等)時的效果和處理時間。另外使用Roboflow調整資料集中之圖像比例至640x640，YOLOv8分別訓練了兩個模型：YOLOv8m及YOLOv8x，後者為YOLOv8最大同時也是最準確的模型。YOLOv9也訓練了兩個模型，分別為YOLOv9-C以及YOLOv9-E，後者是YOLOv9最大且最準確的模型。最後使用PRB-FPN來測試在小物體的檢測中的效果。經過訓練結果以及影片預測之後，所獲得之表現最佳者為YOLOv9-E。在訓練結束後的影片實作中，引入DeepSort技術對已訓練模型檢測到的物體進行連續追蹤，以確保不同幀中識別到的相同物體能夠被視為同一個物體，從而實現有效追蹤。
    為了支持後續專題的進行，在每個物件中還引入了以下屬性:
1.	第一次檢測到物體時的整體畫面:
保存第一次檢測到該物體時的整個畫面，用來進行後續的交通工具抓取。
2.	第一次出現該物件時該物件在整體畫面的位置:
紀錄物體首次出現時在畫面中的位置，用來計算與交通工具的歐式距離。
3.	該幀在整體影片中的幀數:
紀錄當前幀在整個影片中的位置，用於計算掉落速度時所需的幀數差。
4.	該幀的物件中心Y值:
紀錄物體中心的Y座標，用於計算掉落速度時所需的Y值距離差。
5.	該幀距離最近的交通工具在整體畫面上的位置:
紀錄該幀中距離最近的交通工具的位置，以便最終裁切出交通工具的畫面，供車牌辨識API使用。
6.	最近交通工具的面積:
紀錄交通工具在畫面中所佔的面積比例，供動態調整判定閥值的大小。
7.	動態判定閥值:
用於檢測垃圾是否處於掉落狀態。
   這些屬性將被詳細記錄，並用於後續的分析和專題討論中，確保系統的準確性和可靠性。
    為了評估物體是否處於掉落狀態，本專題設置了動態判定閥值。當物件在檢測到兩次時(不須一定為連續幀)，若該物件的Y值距離差/幀數的數量差超過動態判定閥值，則認定物件處於掉落狀態。
    動態判定閥值的設定:由於攝影機距離交通工具的遠近會影響Y值在整體畫面上的高度變化，因此本專題在設定動態判定閥值時，會先檢測該物件在第一幀時距離最近的交通工具，根據這台交通工具所佔的面積比例來調整閥值的大小。若面積小，代表距離較遠，因此動態判定閥值會相對較低。此外，由於拍攝者自身也可能同時在移動，物件在整體畫面上的位置也會變動。因此，當偵測到該物件超過一定幀數後，動態判定閥值會設得較高，以避免誤偵測。
    交通工具的檢測:透過調用訓練好的YOLOv8車輛模型，在掉落物件的第一幀中識別交通工具。如果檢測到多台車輛，會使用歐幾里得距離算法計算每輛車與物件的第一幀位置之間的距離，然後選擇距離最近的車輛，將該交通工具的位置跟面積比例回傳給DeepSort該物件的屬性中。   
   當認定該物件處於掉落狀態後，本專題會讀取該物件屬性中的第一幀的畫面和交通工具在整體畫面上的位置，先裁切該車輛的照片，再使用車牌識別API，將車輛照片傳入已獲取車牌號碼。最終，將車輛的車牌資訊和掉落物體的ID紀錄在列表中，以供舉報或警示使用。這一流程使本專題能夠精確定位涉及人為丟棄垃圾的交通工具，有效維護市容並降低安全隱患。
